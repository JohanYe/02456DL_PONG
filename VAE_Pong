# -*- coding: utf-8 -*-
"""Framestack VAE integration V1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FFLgcd7oSqmwfYaPZtBs26aKXsDt1Wka
"""

import gym
import matplotlib.pyplot as plt
import random
from matplotlib import animation
from collections import deque
import time
import os
import cv2

import torch.nn as nn
import torch
cuda = torch.cuda.is_available()
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.data.sampler import SubsetRandomSampler
from torchvision.datasets import MNIST
from torch.autograd import Variable
from torchvision.transforms import ToTensor
from functools import reduce
import torch.nn.functional as F

import wrappers_VAE

import numpy as np
 
# preprocessing used by Karpathy (cf. https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)
def prepro(I):
    """ prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector """
    I = I[35:195] # crop
    I = I[::2,::2,0] # downsample by factor of 2
    I[I == 144] = 0 # erase background (background type 1)
    I[I == 109] = 0 # erase background (background type 2)
    I[I != 0] = 1 # everything else (paddles, ball) just set to 1
    return I.astype(np.float)

class ReplayMemory(object):
    """Experience Replay Memory"""
     
    def __init__(self, capacity):
        #self.size = size
        self.memory = deque(maxlen=capacity)
     
    def add(self, *args):
        """Add experience to memory."""
        self.memory.append([*args])
     
    def sample(self, batch_size):
        """Sample batch of experiences from memory with replacement."""
        return random.sample(self.memory, batch_size)
     
    def count(self):
        return len(self.memory)

def sample_action():
    n = np.random.uniform(0,1)
    if n <= 1/3:
        return 0
    elif n <= 2/3:
        return 2
    else:
        return 3

#https://github.com/openai/gym/issues/1280
#env = gym.make("Pong-v4")
env = wrappers_VAE.make_atari("PongNoFrameskip-v4")
k = 2 #Number of frames stacked
replay_memory_capacity = 150000
prefill_memory = True
initial_memory_size = replay_memory_capacity
bs = 1500
 
replay_memory = ReplayMemory(replay_memory_capacity)
frame_counter = 0
 
state, state1 = deque(maxlen=k),deque(maxlen=k)
 
# prefill replay memory with random actions
if prefill_memory:
    print('prefill replay memory')
    env.reset()
    for i in range(18):
        #Because the first frames the game is not initiated.
        state_raw, _, _, _ = env.step(0)
        state.append(prepro(state_raw))
        state1.append(prepro(state_raw))
 
    while replay_memory.count() < initial_memory_size:
        action = sample_action()
        # _, reward, done, _ = env.step(action)
        state_raw, reward, done, info = env.step(action)
        state1.append(prepro(state_raw))
        replay_memory.add(np.stack(state), action, reward, np.stack(state1), done)
        #previous input is the raw, state is the previous diff
        if not done:
            state.append(prepro(state_raw))
        else:
            env.reset()
            for i in range(18):
                #Because the first frames the game is not initiated.
                state_raw, _, _, _ = env.step(0)
                state.append(prepro(state_raw))
                state1.append(prepro(state_raw))
        if replay_memory.count() % 5000 == 0:
            print(replay_memory.count())

X_train = wrappers_VAE.dataloader_from_replay_mem(replay_memory,bs)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
VAE = wrappers_VAE.VAE_preprocess().to(device)
optimizer = optim.Adam(VAE.parameters(),lr=0.0001)
epochs = 100

recon_x, batch, mean_loss, VAE = wrappers_VAE.train(VAE,X_train,optimizer,epochs,beta=0)

wrappers_VAE.show_recon_x(recon_x,batch)

with torch.no_grad():
    z, _, _ = VAE.encode(batch)

fig, axs = plt.subplots(4, 4,figsize=(7,7))
z_plot = z.cpu().numpy()
for i in range(4):
    for j in range(4):
        axs[i,j].imshow(z_plot[4*i+j][0])
        axs[i,j].axis('off')

#https://github.com/openai/gym/issues/1280
#env = gym.make("Pong-v4")
env = wrappers_VAE.make_atari("PongNoFrameskip-v4")
k = 2 #Number of frames stacked (VAE outputs 3 'frames')
replay_memory_capacity = 50000
prefill_memory = True
initial_memory_size = 0.1*replay_memory_capacity
bs = 250
 
replay_memory = ReplayMemory(replay_memory_capacity)
frame_counter = 0
 
state, state1 = deque(maxlen=k),deque(maxlen=k)
 
# prefill replay memory with random actions
# STATE = FRAME DIFFS, PREV/CUR_INPUT = STILL FRAMES
if prefill_memory:
    print('prefill replay memory')
    env.reset()
    for i in range(18):
        #Because the first frames the game is not initiated.
        state_raw, _, _, _ = env.step(0)
        state_raw = torch.from_numpy(prepro(state_raw)).unsqueeze(0).unsqueeze(0).float().to(device)
        state_raw = VAE.encode(state_raw)[0].detach().cpu().squeeze(0)
        state.append(state_raw)
        state1.append(state_raw)
 
    while replay_memory.count() < initial_memory_size:
        action = sample_action()
        # _, reward, done, _ = env.step(action)
        state_raw, reward, done, info = env.step(action)
        state_raw = torch.from_numpy(prepro(state_raw)).unsqueeze(0).unsqueeze(0).float().to(device)
        state_raw = VAE.encode(state_raw)[0].detach().cpu().squeeze(0)
        state1.append(state_raw)
        replay_memory.add(np.concatenate((state[0],state[1])), action, reward,np.concatenate((state1[0],state1[1])), done)
        #previous input is the raw, state is the previous diff
        if not done:
            state.append(state_raw)
        else:
            env.reset()
            for i in range(18):
                #Because the first frames the game is not initiated.
                state_raw, _, _, _ = env.step(0)
                state_raw = torch.from_numpy(prepro(state_raw)).unsqueeze(0).unsqueeze(0).float().to(device)
                state_raw = VAE.encode(state_raw)[0].detach().cpu().squeeze(0)
                state.append(state_raw)
                state1.append(state_raw)

class DQN(nn.Module):
    def __init__(self, input_features = 2304, output_layer = 3):
        super(DQN,self).__init__()
        
        self.out = nn.Sequential(
            nn.Linear(input_features, 512),
            nn.ReLU(),
            nn.Linear(512,output_layer)
        )

        self.convolutions = nn.Sequential(
            nn.Conv2d(in_channels = 6, out_channels=16, kernel_size = 4, stride = 1),
            nn.ReLU(),
            nn.Conv2d(16,32,3,1,padding=1),
            nn.ReLU(),
            nn.Conv2d(32,64,2,1),
            nn.ReLU(),
        )
    
    def forward(self,x):
        x = self.convolutions(x)
        x = x.view(x.size(0), -1)
        #print(x.size())
        out = self.out(x)
        return out

    #sum squared diff
    def loss(self, q_outputs, q_targets):
        return torch.sum(torch.pow(q_targets - q_outputs, 2))
    
    def update_params(self,new_params,tau):
        params = self.state_dict()
        for k in params.keys():
            params[k] = (1-tau) * params[k] + tau * new_params[k]
        self.load_state_dict(params)

num_episodes = 1050
max_frames = 1125
#episode_explore = 10
epsilon = 1.0
rewards, lengths, losses, epsilons = [], [], [], []
x_train, y_train, rewards = [],[],[]
batch_size = 32
prev_input = np.zeros((80,80))
reward_sum = 0
n_outputs = 3
cwd = os.getcwd()
 
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
 
# initialize DQN and replay memory
policy_dqn = DQN().to(device)
policy_optimizer = optim.Adam(policy_dqn.parameters(),lr=0.0001)
 
target_dqn = DQN().to(device)
target_dqn.load_state_dict(policy_dqn.state_dict())
 
epsilon = 1
rewards, lengths, losses, epsilons = [], [], [], []

gamma = 0.99 # discount rate
tau = 0.01 # target network update rate
val_freq = 100 # validation frequency
action_counter = {'0':0,'2':0,'3':0}
reward_log = {}
capture = False
frames = []
play_steps = 4
epsilon_end = 0.02
VAE_train_timer = 0

start_time = time.time()
k = 2
s, s1 = deque(maxlen=k),deque(maxlen=k)

for episode in range(num_episodes):

    ep_reward, ep_loss = 0,0
    env.reset()
    for i in range(18):
        s_raw,_,_,_ = env.step(0)
        s_raw = torch.from_numpy(prepro(s_raw)).unsqueeze(0).unsqueeze(0).float().to(device)
        s_raw = VAE.encode(s_raw)[0].detach().cpu().squeeze(0)
        s.append(s_raw)
        s1.append(s_raw)

    #if save replay
    if capture: frames.append(s_raw)
    
    frame_counter = 1
     
    for frame in range(max_frames):
 
        for i in range(play_steps):
            #Explore vs exploit
            if np.random.rand() < epsilon:
                a = sample_action()
            else:
                with torch.no_grad():
                    s_tmp = torch.from_numpy(np.concatenate((s[0],s[1]))).unsqueeze(0).float().to(device)
                    # z, _, _ = VAE.encode(s_tmp,stack=True)
                    a = policy_dqn(s_tmp).argmax().item()
                    a +=1 if a != 0 else a

            # perform action
            action_counter[str(a)] += 1
            # _, r, done, _ = env.step(a)
            # ep_reward += r
            s1_raw, r, done, _ = env.step(a)
            s1_raw = torch.from_numpy(prepro(s1_raw)).unsqueeze(0).unsqueeze(0).float().to(device)
            s1_raw = VAE.encode(s1_raw)[0].detach().cpu().squeeze(0)
            #if save replay
            if capture: frames.append(s1_raw)

            s1.append(s1_raw)

            # store experience in replay memory
            replay_memory.add(np.concatenate((s[0],s[1])), a, r, np.concatenate((s1[0],s1[1])), done)
                    
            s.append(s1_raw)
            ep_reward += r
            frame_counter += 1
            VAE_train_timer +=1            
            if done:
                break
 
        if replay_memory.count() >= batch_size:
             
            # sample batch from replay memory
            batch = replay_memory.sample(batch_size) #shape: [batch_size, 5]
            for i in range(play_steps-1):
                batch.extend(replay_memory.sample(batch_size))
                
            batch = np.array(batch)
            ss, aa, rr, ss1, dd = batch[:,0], batch[:,1], batch[:,2], batch[:,3], batch[:,4]
 
            # do forward pass of batch
            policy_optimizer.zero_grad()
            ss = torch.from_numpy(np.stack(ss)).float().to(device)
            # z, _, _ = VAE.encode(ss,stack=True)
            Q = policy_dqn(ss)
 
            with torch.no_grad():
                # TODO: use target net
                ss1 = torch.from_numpy(np.stack(ss1)).float().to(device)
                # z1, _, _ = VAE.encode(ss1,stack=True)
                Q1 = target_dqn(ss1)
             
            # compute target for each sampled experience
            q_targets = Q.clone()
 
            #calculate q_targets 
            for k in range(batch_size):
                aa[k] -=1 if aa[k] != 0 else aa[k]
                q_targets[k, aa[k]] = rr[k] + gamma * Q1[k].max().item() * (not dd[k])
                 
            # update network weights
            loss = policy_dqn.loss(Q, q_targets)
            loss.backward()
            policy_optimizer.step()
            # update target network parameters from policy network parameters
            target_dqn.update_params(policy_dqn.state_dict(), tau)

            #update VAE every 2000 frames
            if VAE_train_timer > 100000000:
                X_train = wrappers_VAE.dataloader_from_replay_mem(replay_memory,bs)
                _,_,_,VAE = wrappers_VAE.train(VAE,X_train,optimizer,epochs = 10,beta=1)
                VAE_train_timer = 0

        ep_loss += loss.item()
        if done: 
            break
 
    #Logging
    reward_log[str(episode)] = ep_reward
    
    if epsilon > epsilon_end:
        epsilon *= 0.9935
    else:
        epsilon = epsilon_end
    
    epsilons.append(epsilon); rewards.append(ep_reward); lengths.append(frame+1); losses.append(ep_loss) #bookkeping
     
    #Printing
    print('{} EPISODE: {}:, reward:{}, frames: {}, epsilon: {}, last_{}_avgR: {}'.format(
        time.strftime('%X %x'),episode,ep_reward,frame_counter,epsilon,100,round(np.mean(rewards[-100:]),2)))
 
    # saving gif
    if capture:
        gif = cwd + '/Pong_gifs/FrameStack/pong_epoch_' + str(episode) + '.gif'
        #save_frames_as_gif(frames, filename=gif)
        capture = False
        frames = []
 
    #Change validation frequency, if model performs well.
    if ep_reward > 0:
        val_freq = 50
 
    #capture gif
    if (episode+1) % (val_freq) == 0:
        #capturing
        frames, capture = [], True
        
    if int(time.time()-start_time) > 82800:
        break

s_tmp.shape

outfile = open('VAE_log.txt','w')
print(epsilons,file=outfile)
print(lengths,file=outfile)
print(rewards,file=outfile)
print(losses,file=outfile)
outfile.close()

import seaborn as sns
x_vals = list(map(int,reward_log.keys()))
y_vals = [reward_log[str(k)] for k in x_vals]

fig,ax = plt.subplots(figsize=(20,10))
#plt.locator_params(axis='x',nbins=10)
ax.set_xlabel('Episode')
ax.set_ylabel('reward')

ax.plot(x_vals,y_vals)
#plt.show()
fig.savefig('frame_stack_reward.png')

