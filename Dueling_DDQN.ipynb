{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Kopi af Kopi af DuelDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRh7s0R8khkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from google.colab import drive\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from matplotlib import animation\n",
        "from collections import deque\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "cuda = torch.cuda.is_available()\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.autograd import Variable\n",
        "from torchvision.transforms import ToTensor\n",
        "from functools import reduce\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QceOjjsbuLNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARIZkKy7kiMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"PongDeterministic-v4\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdMjMxHPkiOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dueling used by Higgsfield (cf. https://github.com/higgsfield/RL-Adventure/blob/master/3.dueling%20dqn.ipynb)\n",
        "def compute_loss(ss, aa, rr, ss1, dd):\n",
        "    for i in range(len(aa)):\n",
        "        if aa[i] > 0: aa[i] -= 1\n",
        "            \n",
        "    state      = Variable(torch.from_numpy(np.stack(ss)).float().to(device))\n",
        "    next_state = Variable(torch.from_numpy(np.stack(ss1)).float().to(device))\n",
        "    action     = Variable(torch.from_numpy(np.stack(aa)).long().to(device))\n",
        "    reward     = Variable(torch.from_numpy(np.stack(rr)).float().to(device))\n",
        "    done       = Variable(torch.from_numpy(np.stack(dd)).float().to(device))\n",
        "\n",
        "    q_values      = policy_dqn(state)\n",
        "    q_values_next = policy_dqn(next_state)\n",
        "    next_q_values = target_dqn(next_state)\n",
        "\n",
        "    q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
        "    next_q_value = next_q_values.gather(1, torch.max(q_values_next, 1)[1].unsqueeze(1)).squeeze(1)\n",
        "\n",
        "    next_q_value     = next_q_values.max(1)[0]\n",
        "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
        "    \n",
        "    loss = (q_value - expected_q_value.detach()).pow(2).mean()\n",
        "        \n",
        "    policy_dqn.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    policy_dqn.optimizer.step()\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9xVnLsfkiQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocessing used by Karpathy (cf. https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)\n",
        "def prepro(I):\n",
        "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "    I = I[35:195] # crop\n",
        "    I = I[::2,::2,0] # downsample by factor of 2\n",
        "    I[I == 144] = 0 # erase background (background type 1)\n",
        "    I[I == 109] = 0 # erase background (background type 2)\n",
        "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "    return I.astype(np.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBL-PZxykiSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory(object):\n",
        "    \"\"\"Experience Replay Memory\"\"\"\n",
        "    \n",
        "    def __init__(self, capacity):\n",
        "        #self.size = size\n",
        "        self.memory = deque(maxlen=capacity)\n",
        "\n",
        "    \n",
        "    def add(self, *args):\n",
        "        \"\"\"Add experience to memory.\"\"\"\n",
        "        self.memory.append([*args])\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Sample batch of experiences from memory with replacement.\"\"\"\n",
        "        return random.sample(self.memory, batch_size)\n",
        "    \n",
        "    def count(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh5iMiH4kiUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "replay_memory_capacity = 50000\n",
        "prefill_memory = True\n",
        "\n",
        "replay_memory = ReplayMemory(replay_memory_capacity)\n",
        "\n",
        "def sample_action():\n",
        "    n = np.random.uniform(0,1)\n",
        "    if n <= 1/3:\n",
        "        return 0\n",
        "    elif n <= 2/3:\n",
        "        return 2\n",
        "    else:\n",
        "        return 3\n",
        "\n",
        "if prefill_memory:\n",
        "\n",
        "    env.reset()\n",
        "    prev_input = np.zeros((80,80))\n",
        "    for i in range(15):\n",
        "        env.step(0)\n",
        "    state0,_,_,_ = env.step(0)\n",
        "    cur_input = prepro(state0)\n",
        "    state = (cur_input - prev_input).reshape(1,80,80)\n",
        "    prev_input = cur_input \n",
        "\n",
        "    while replay_memory.count() <= 0.2*replay_memory_capacity:\n",
        "        action = sample_action()\n",
        "        state_raw, reward, done, _ = env.step(action)\n",
        "        cur_input = prepro(state_raw)\n",
        "        state1 = (cur_input - prev_input).reshape(1,80,80)\n",
        "        replay_memory.add(state, action, reward, state1, done)\n",
        "        if not done:\n",
        "            prev_input,state = cur_input,state1\n",
        "        else:\n",
        "            env.reset()\n",
        "            for i in range(15):\n",
        "                env.step(0)\n",
        "            state_raw,_,_,_ = env.step(0)\n",
        "            prev_input = prepro(state_raw).reshape(1,80,80)\n",
        "            state = prev_input\n",
        "            \n",
        "        if replay_memory.count() % 1000 == 0:\n",
        "            print('replay #:',replay_memory.count())\n",
        "            patch = plt.imshow(state1[0])\n",
        "            plt.axis('off')\n",
        "            plt.colorbar()\n",
        "            plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIviLJpf6PFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Print(nn.Module):\n",
        "    def forward(self, x):\n",
        "        print(x.size())\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "we_t8cWOkiWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DuelingCnnDQN(nn.Module):\n",
        "    def __init__(self, input_features = 1568, output_layer = 3):\n",
        "        super(DuelingCnnDQN,self).__init__()\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(input_features, 512),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "            \n",
        "\n",
        "            \n",
        "        self.advantage = nn.Sequential(\n",
        "            nn.Linear(input_features, 512),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(512, output_layer)\n",
        "        )\n",
        "\n",
        "        self.convolutions = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = 1, out_channels=4, kernel_size = 5, stride = 2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv2d(4,16,5,2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv2d(16,32,5,2),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "    \n",
        "    def forward(self,x):\n",
        "        x = self.convolutions(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        advantage = self.advantage(x)\n",
        "        out = self.out(x)\n",
        "        return out + advantage - advantage.mean()\n",
        "\n",
        "    #sum squared diff\n",
        "    def loss(self, q_outputs, q_targets):\n",
        "        return torch.sum(torch.pow(q_targets - q_outputs, 2))\n",
        "    \n",
        "    def update_params(self,new_params,tau):\n",
        "        params = self.state_dict()\n",
        "        for k in params.keys():\n",
        "            params[k] = (1-tau) * params[k] + tau * new_params[k]\n",
        "        self.load_state_dict(params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yrsFQ9RkiYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_episodes = 1000\n",
        "episode_limit = 1100\n",
        "batch_size = 32\n",
        "epsilon = 1\n",
        "epsilon_start = 1\n",
        "epsilon_end = 0.02\n",
        "epsilon_decay_rate = 150*1000\n",
        "frames_list = []\n",
        "epsilon_greedy = 1/(num_episodes*0.5)\n",
        "\n",
        "\n",
        "step_play = 4\n",
        "gamma = 0.99 # discount rate\n",
        "tau = 0.01 # target network update rate\n",
        "replay_memory_capacity = 10000\n",
        "prefill_memory = True\n",
        "val_freq = 50 # validation frequency\n",
        "capture = True\n",
        "\n",
        "\n",
        "# initialize DQN and replay memory\n",
        "policy_dqn = DuelingCnnDQN().to(device)\n",
        "target_dqn = DuelingCnnDQN().to(device)\n",
        "\n",
        "target_dqn.load_state_dict(policy_dqn.state_dict())\n",
        "\n",
        "policy_dqn.optimizer = optim.Adam(policy_dqn.parameters(),lr=0.0001)\n",
        "\n",
        "rewards, lengths, losses, epsilons = [], [], [], []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDEA4rUyuHNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load = False\n",
        "\n",
        "if load:\n",
        "    model_save_name = 'classifier.pt'\n",
        "    path = F\"/content/gdrive/My Drive/\" + model_save_name\n",
        "\n",
        "    checkpoint = torch.load(path)\n",
        "    policy_dqn.load_state_dict(checkpoint['policy_dqn'])\n",
        "    target_dqn.load_state_dict(checkpoint['target_dqn'])\n",
        "    policy_dqn.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    replay_memory = checkpoint['replay_mem']\n",
        "    episode = checkpoint['epoch']\n",
        "    epsilon = checkpoint['epsilon']\n",
        "    loss = checkpoint['loss']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtozS4mJm62g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_frames_as_gif(frames, filename=None):\n",
        "    \"\"\"\n",
        "    Save a list of frames as a gif\n",
        "    \"\"\"\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    if filename:\n",
        "        anim.save(filename, dpi=72, writer='imagemagick')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-aU5bchkrPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for episode in range(num_episodes):\n",
        "\n",
        "    if load:\n",
        "       episode += checkpoint['epoch']\n",
        "    \n",
        "    frames = 0\n",
        "    ep_reward, ep_loss = 0, 0\n",
        "    # init new episode\n",
        "    env.reset()\n",
        "    prev_input = np.zeros((80,80))\n",
        "    for i in range(15):\n",
        "        env.step(0)\n",
        "    state0,_,_,_ = env.step(0)\n",
        "    cur_input = prepro(state0)\n",
        "    state = (cur_input - prev_input).reshape(1,80,80)\n",
        "    prev_input = cur_input\n",
        "\n",
        "    #append frames to create a giffy\n",
        "    if capture: frames_list.append(state0)\n",
        "\n",
        "    for frame in range(episode_limit):\n",
        "        # select action with epsilon-greedy strategy\n",
        "        for step in range(step_play):\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = sample_action()\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    s_tmp = torch.from_numpy(state.reshape(1,1,80,80)).float().to(device)\n",
        "                    action = policy_dqn(s_tmp).argmax().item()\n",
        "                    action +=1 if action != 0 else action \n",
        "\n",
        "            state_raw, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Append frames to giffy\n",
        "            if capture: frames_list.append(state_raw)\n",
        "\n",
        "            cur_input = prepro(state_raw)\n",
        "            state1 = (cur_input - prev_input).reshape(1,80,80)\n",
        "            replay_memory.add(state, action, reward, state1, done)\n",
        "        \n",
        "            state = state1\n",
        "            prev_input = cur_input\n",
        "            ep_reward += reward\n",
        "            frames += step\n",
        "            if done:\n",
        "                break\n",
        "                \n",
        "        if replay_memory.count() >= batch_size:\n",
        "            batch = replay_memory.sample(batch_size)\n",
        "            for step in range(step_play):\n",
        "                batch.extend(replay_memory.sample(batch_size))\n",
        "            batch = np.array(batch)\n",
        "            ss, aa, rr, ss1, dd = batch[:,0], batch[:,1], batch[:,2], batch[:,3], batch[:,4]\n",
        "            loss = compute_loss(ss, aa, rr, ss1, dd)\n",
        "            target_dqn.update_params(policy_dqn.state_dict(), tau)\n",
        "        ep_loss += loss.item()                 \n",
        "        if done: \n",
        "            break\n",
        "    if epsilon > 0.01:\n",
        "        epsilon *= 0.9935\n",
        "    else:\n",
        "        epsilon = 0.01\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    rewards.append(ep_reward)\n",
        "    print('{} Episode:, {}, reward:,{}, frames: ,{}, epsilon: ,{}, last_{}_avgR: ,{}'.format(\n",
        "        time.strftime('%X %x'),episode+1, ep_reward, frames, round(epsilon,3), 100,np.mean(rewards[-100:])))\n",
        "\n",
        "     # save a gif\n",
        "    if capture:\n",
        "        gif = F\"/content/gdrive/My Drive/Pong_gifs\" + str(episode) + '.gif'\n",
        "        save_frames_as_gif(frames_list, filename=gif)\n",
        "        capture = False\n",
        "        frames = []\n",
        "\n",
        "    # frequency for when to make a giffy\n",
        "    if (episode+1) % val_freq == 0: # Test setup\n",
        "        frames_list, capture = [], True\n",
        "        print('GIF saved')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98yKXIARk2_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_save_name = 'classifier.pt'\n",
        "path = F\"/content/gdrive/My Drive/\" + model_save_name\n",
        "torch.save({\n",
        "    'epoch': episode +1,\n",
        "    'policy_dqn': policy_dqn.state_dict(),\n",
        "    'target_dqn': target_dqn.state_dict(),\n",
        "    'optimizer': policy_dqn.optimizer.state_dict(),\n",
        "    'replay_mem': replay_memory,\n",
        "    'epsilon': epsilon,\n",
        "    'loss': loss\n",
        "}, path)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}