# -*- coding: utf-8 -*-
"""Pong_project_Convolutions_framestack_v1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aZ4nIYlgWNa6YFPRbZQnlFLMHoKGbl8s
"""

import gym
import matplotlib.pyplot as plt
import random
from matplotlib import animation
from collections import deque
import time
import os
import cv2

def save_frames_as_gif(frames, filename=None):
    """
    Save a list of frames as a gif
    """
    patch = plt.imshow(frames[0])
    plt.axis('off')
    def animate(i):
        patch.set_data(frames[i])
    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)
    if filename:
        anim.save(filename, dpi=72, writer='imagemagick')

import numpy as np
 
# preprocessing used by Karpathy (cf. https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)
def prepro(I):
    """ prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector """
    I = I[35:195] # crop
    I = I[::2,::2,0] # downsample by factor of 2
    I[I == 144] = 0 # erase background (background type 1)
    I[I == 109] = 0 # erase background (background type 2)
    I[I != 0] = 1 # everything else (paddles, ball) just set to 1
    return I.astype(np.float)

# import numpy as np
 
# # preprocessing used by Karpathy (cf. https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)
# def prepro(I):
#     """ prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector """
#     I = I[:, :, 0] * 0.299 + I[:, :, 1] * 0.587 + I[:, :, 2] * 0.114
#     resized_screen = cv2.resize(I, (80, 110), interpolation=cv2.INTER_AREA)
#     x_t = resized_screen[20:100, :]
#     x_t = np.reshape(x_t, [80, 80])
#     return x_t.astype(np.float)

import torch.nn as nn
import torch
cuda = torch.cuda.is_available()
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.data.sampler import SubsetRandomSampler
from torchvision.datasets import MNIST
from torch.autograd import Variable
from torchvision.transforms import ToTensor
from functools import reduce
import torch.nn.functional as F

class ReplayMemory(object):
    """Experience Replay Memory"""
     
    def __init__(self, capacity):
        #self.size = size
        self.memory = deque(maxlen=capacity)
     
    def add(self, *args):
        """Add experience to memory."""
        self.memory.append([*args])
     
    def sample(self, batch_size):
        """Sample batch of experiences from memory with replacement."""
        return random.sample(self.memory, batch_size)
     
    def count(self):
        return len(self.memory)

# Atrari wrappers https://github.com/Shmuma/ptan/blob/master/samples/dqn_speedup/lib/atari_wrappers.py
class NoopResetEnv(gym.Wrapper):
    def __init__(self, env, noop_max=30):
        """Sample initial states by taking random number of no-ops on reset.
        No-op is assumed to be action 0.
        """
        gym.Wrapper.__init__(self, env)
        self.noop_max = noop_max
        self.override_num_noops = None
        self.noop_action = 0
        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'

    def _reset(self, **kwargs):
        """ Do no-op action for a number of steps in [1, noop_max]."""
        self.env.reset(**kwargs)
        if self.override_num_noops is not None:
            noops = self.override_num_noops
        else:
            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101
        assert noops > 0
        obs = None
        for _ in range(noops):
            obs, _, done, _ = self.env.step(self.noop_action)
            if done:
                obs = self.env.reset(**kwargs)
        return obs

class EpisodicLifeEnv(gym.Wrapper):
    def __init__(self, env=None):
        """Make end-of-life == end-of-episode, but only reset on true game over.
        Done by DeepMind for the DQN and co. since it helps value estimation.
        """
        super(EpisodicLifeEnv, self).__init__(env)
        self.lives = 0
        self.was_real_done = True
        self.was_real_reset = False

    def _step(self, action):
        obs, reward, done, info = self.env.step(action)
        self.was_real_done = done
        # check current lives, make loss of life terminal,
        # then update lives to handle bonus lives
        lives = self.env.unwrapped.ale.lives()
        if lives < self.lives and lives > 0:
            # for Qbert somtimes we stay in lives == 0 condtion for a few frames
            # so its important to keep lives > 0, so that we only reset once
            # the environment advertises done.
            done = True
        self.lives = lives
        return obs, reward, done, info

    def _reset(self):
        """Reset only when lives are exhausted.
        This way all states are still reachable even though lives are episodic,
        and the learner need not know about any of this behind-the-scenes.
        """
        if self.was_real_done:
            obs = self.env.reset()
            self.was_real_reset = True
        else:
            # no-op step to advance from terminal/lost life state
            obs, _, _, _ = self.env.step(0)
            self.was_real_reset = False
        self.lives = self.env.unwrapped.ale.lives()
        return obs


class FireResetEnv(gym.Wrapper):
    def __init__(self, env=None):
        """For environments where the user need to press FIRE for the game to start."""
        super(FireResetEnv, self).__init__(env)
        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'
        assert len(env.unwrapped.get_action_meanings()) >= 3

    def _reset(self):
        self.env.reset()
        obs, _, done, _ = self.env.step(1)
        if done:
            self.env.reset()
        obs, _, done, _ = self.env.step(2)
        if done:
            self.env.reset()
        return obs


class MaxAndSkipEnv(gym.Wrapper):
    def __init__(self, env=None, skip=4):
        """Return only every `skip`-th frame"""
        super(MaxAndSkipEnv, self).__init__(env)
        # most recent raw observations (for max pooling across time steps)
        self._obs_buffer = deque(maxlen=2)
        self._skip = skip

    def step(self, action):
        total_reward = 0.0
        done = None
        for _ in range(self._skip):
            obs, reward, done, info = self.env.step(action)
            self._obs_buffer.append(obs)
            total_reward += reward
            if done:
                break

        max_frame = np.max(np.stack(self._obs_buffer), axis=0)

        return max_frame, total_reward, done, info

    def reset(self):
        """Clear past frame buffer and init. to first obs. from inner env."""
        self._obs_buffer.clear()
        obs = self.env.reset()
        self._obs_buffer.append(obs)
        return obs
        
class LazyFrames(object):
    def __init__(self, frames):
        """This object ensures that common frames between the observations are only stored once.
        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay
        buffers.
        This object should only be converted to numpy array before being passed to the model.
        You'd not belive how complex the previous solution was."""
        self._frames = frames

    def __array__(self, dtype=None):
        out = np.concatenate(self._frames, axis=0)
        if dtype is not None:
            out = out.astype(dtype)
        return out

def make_atari(env_id):
    env = gym.make(env_id)
    assert 'NoFrameskip' in env.spec.id
    if 'FIRE' in env.unwrapped.get_action_meanings():
        env = FireResetEnv(env)
    env = EpisodicLifeEnv(env)
    env = NoopResetEnv(env, noop_max=30)
    env = MaxAndSkipEnv(env, skip=4)
    return env

#https://github.com/openai/gym/issues/1280
#env = gym.make("Pong-v4")
env = make_atari("PongNoFrameskip-v4")
k = 2 #Number of frames stacked
replay_memory_capacity = 50000
prefill_memory = True
initial_memory_size = 0.2*replay_memory_capacity
 
replay_memory = ReplayMemory(replay_memory_capacity)
frame_counter = 0
 
state, state1 = deque(maxlen=k),deque(maxlen=k)
 
def sample_action():
    n = np.random.uniform(0,1)
    if n <= 1/3:
        return 0
    elif n <= 2/3:
        return 2
    else:
        return 3
 
# prefill replay memory with random actions
# STATE = FRAME DIFFS, PREV/CUR_INPUT = STILL FRAMES
if prefill_memory:
    print('prefill replay memory')
    env.reset()
    for i in range(18):
        #Because the first frames the game is not initiated.
        state_raw, _, _, _ = env.step(0)
        state.append(prepro(state_raw))
        state1.append(prepro(state_raw))
 
    while replay_memory.count() < initial_memory_size:
        action = sample_action()
        # _, reward, done, _ = env.step(action)
        state_raw, reward, done, info = env.step(action)
        state1.append(prepro(state_raw))
        replay_memory.add(np.stack(state), action, reward, np.stack(state1), done)
        #previous input is the raw, state is the previous diff
        if not done:
            state.append(prepro(state_raw))
        else:
            env.reset()
            for i in range(18):
                #Because the first frames the game is not initiated.
                state_raw, _, _, _ = env.step(0)
                state.append(prepro(state_raw))
                state1.append(prepro(state_raw))

class DQN(nn.Module):
    def __init__(self, input_features = 2304, output_layer = 3):
        super(DQN,self).__init__()
        
        self.out = nn.Sequential(
            nn.Linear(input_features, 512),
            nn.ReLU(),
            nn.Linear(512,output_layer)
        )

        self.convolutions = nn.Sequential(
            nn.Conv2d(in_channels = 2, out_channels=16, kernel_size = 8, stride = 4),
            nn.ReLU(),
            nn.Conv2d(16,32,4,2),
            nn.ReLU(),
            nn.Conv2d(32,64,3,1),
            nn.ReLU()
        )
    
    def forward(self,x):
        x = self.convolutions(x)
        x = x.view(x.size(0), -1)
        #print(x.size())
        out = self.out(x)
        return out

    #sum squared diff
    def loss(self, q_outputs, q_targets):
        return torch.sum(torch.pow(q_targets - q_outputs, 2))
    
    def update_params(self,new_params,tau):
        params = self.state_dict()
        for k in params.keys():
            params[k] = (1-tau) * params[k] + tau * new_params[k]
        self.load_state_dict(params)

def Wrap_modelstate(epoch,model,optimizer,epsilon,target_network,replay):
    Model_dict = {'epoch': epoch + 1,
                  'epsilon': epsilon,
                  'state_dict': model.state_dict(),
                  'optimizer': optimizer.state_dict(),
                  'target_network':target_network.state_dict(),
                  'replay_buffer':replay}
    return Model_dict

num_episodes = 1200
max_frames = 1125
#episode_explore = 10
epsilon = 1.0
rewards, lengths, losses, epsilons = [], [], [], []
x_train, y_train, rewards = [],[],[]
batch_size = 32
prev_input = np.zeros((80,80))
reward_sum = 0
n_outputs = 3
cwd = os.getcwd()
 
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
 
# initialize DQN and replay memory
policy_dqn = DQN().to(device)
policy_optimizer = optim.Adam(policy_dqn.parameters(),lr=0.0001)
 
target_dqn = DQN().to(device)
target_dqn.load_state_dict(policy_dqn.state_dict())
 
epsilon = 1
rewards, lengths, losses, epsilons = [], [], [], []

test = deque(maxlen=2)
test.append(1)
test.append(2)
test.append(3)
print(test)

gamma = 0.99 # discount rate
tau = 0.01 # target network update rate
val_freq = 100 # validation frequency
action_counter = {'0':0,'2':0,'3':0}
reward_log = {}
capture = False
frames = []
play_steps = 4
epsilon_end = 0.02

start_time = time.time()
s, s1 = deque(maxlen=k),deque(maxlen=k)
 
for episode in range(num_episodes):

    ep_reward, ep_loss = 0,0
    env.reset()
    for i in range(18):
        s_raw,_,_,_ = env.step(0)
        s.append(prepro(s_raw))
        s1.append(prepro(s_raw))

    #if save replay
    if capture: frames.append(s_raw)
    
    frame_counter = 1
     
    for frame in range(max_frames):
 
        for i in range(play_steps):
            #Explore vs exploit
            if np.random.rand() < epsilon:
                a = sample_action()
            else:
                with torch.no_grad():
                    s_tmp = torch.from_numpy(np.stack(s)).unsqueeze(0).float().to(device)
                    a = policy_dqn(s_tmp).argmax().item()
                    a +=1 if a != 0 else a

            # perform action
            action_counter[str(a)] += 1
            # _, r, done, _ = env.step(a)
            # ep_reward += r
            s1_raw, r, done, _ = env.step(a)
            #if save replay
            if capture: frames.append(s1_raw)

            s1.append(prepro(s1_raw))

            # store experience in replay memory
            replay_memory.add(np.stack(s), a, r, np.stack(s1), done)
                    
            s.append(prepro(s1_raw))
            ep_reward += r
            frame_counter += 1
            
            if done:
                break
 
        if replay_memory.count() >= batch_size:
             
            # sample batch from replay memory
            batch = replay_memory.sample(batch_size) #shape: [batch_size, 5]
            for i in range(play_steps-1):
                batch.extend(replay_memory.sample(batch_size))
                
            batch = np.array(batch)
            #Since we sample from replay buffer, and it is filled with state_differences, we don't need to worry about subtraction here.
            ss, aa, rr, ss1, dd = batch[:,0], batch[:,1], batch[:,2], batch[:,3], batch[:,4]
 
            # do forward pass of batch
            policy_optimizer.zero_grad()
            ss = torch.from_numpy(np.stack(ss)).float().to(device)
            Q = policy_dqn(ss)
 
            with torch.no_grad():
                # TODO: use target net
                ss1 = torch.from_numpy(np.stack(ss1)).float().to(device)
                Q1 = target_dqn(ss1)
             
            # compute target for each sampled experience
            q_targets = Q.clone()
 
            #calculate q_targets 
            for k in range(batch_size):
                aa[k] -=1 if aa[k] != 0 else aa[k]
                q_targets[k, aa[k]] = rr[k] + gamma * Q1[k].max().item() * (not dd[k])
                 
            # update network weights
            loss = policy_dqn.loss(Q, q_targets)
            loss.backward()
            policy_optimizer.step()
            # update target network parameters from policy network parameters
            target_dqn.update_params(policy_dqn.state_dict(), tau)

        ep_loss += loss.item()
        if done: 
            break
 
    #Logging
    reward_log[str(episode)] = ep_reward
    
    if epsilon > epsilon_end:
        epsilon *= 0.9935
    else:
        epsilon = epsilon_end
    
    epsilons.append(epsilon); rewards.append(ep_reward); lengths.append(frame+1); losses.append(ep_loss) #bookkeping
     
    #Printing
    print('{} EPISODE: {}:, reward:{}, frames: {}, epsilon: {}, last_{}_avgR: {}'.format(
        time.strftime('%X %x'),episode,ep_reward,frame_counter,epsilon,100,round(np.mean(rewards[-100:]),2)))
 
    # saving gif
    if capture:
        gif = cwd + '/Pong_gifs/FrameStack/pong_epoch_' + str(episode) + '.gif'
        #save_frames_as_gif(frames, filename=gif)
        capture = False
        frames = []
 
    #Change validation frequency, if model performs well.
    if ep_reward > 0:
        val_freq = 50
 
    #capture gif
    if (episode+1) % (val_freq) == 0:
        #capturing
        frames, capture = [], True
        
    if int(time.time()-start_time) > 82800:
        break

e = 1

e

outfile = open('framestack_log.txt','w')
print(epsilons,file=outfile)
print(lengths,file=outfile)
print(rewards,file=outfile)
print(losses,file=outfile)
outfile.close()

import seaborn as sns
x_vals = list(map(int,reward_log.keys()))
y_vals = [reward_log[str(k)] for k in x_vals]

fig,ax = plt.subplots(figsize=(20,10))
#plt.locator_params(axis='x',nbins=10)
ax.set_xlabel('Episode')
ax.set_ylabel('reward')

ax.plot(x_vals,y_vals)
#plt.show()
fig.savefig('frame_stack_reward.png')

